{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"taxi_demo.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"AknnZ8x4LZ0b","executionInfo":{"status":"ok","timestamp":1624353520285,"user_tz":-60,"elapsed":854,"user":{"displayName":"Ivana Dusparic","photoUrl":"","userId":"00662899544751628971"}}},"source":["# Inspired from https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n","import os\n","import time\n","import gym\n","import numpy as np\n","from tqdm.notebook import trange, tqdm\n","from IPython.display import clear_output"],"id":"AknnZ8x4LZ0b","execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sy1904_9LZ0c"},"source":["# Taxi-v3\n","This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions."],"id":"Sy1904_9LZ0c"},{"cell_type":"code","metadata":{"id":"kjyjhPfKLZ0d","executionInfo":{"status":"ok","timestamp":1624353530248,"user_tz":-60,"elapsed":723,"user":{"displayName":"Ivana Dusparic","photoUrl":"","userId":"00662899544751628971"}}},"source":["env = gym.make(\"Taxi-v3\").env"],"id":"kjyjhPfKLZ0d","execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q3NhKiGeLZ0d"},"source":["![Taxi environment](images/taxi_env.png)"],"id":"Q3NhKiGeLZ0d"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C75lCsFpLZ0d","executionInfo":{"status":"ok","timestamp":1624353532200,"user_tz":-60,"elapsed":201,"user":{"displayName":"Ivana Dusparic","photoUrl":"","userId":"00662899544751628971"}},"outputId":"4d17b862-38aa-4f21-8d49-7e420e4bba9b"},"source":["env.render()"],"id":"C75lCsFpLZ0d","execution_count":3,"outputs":[{"output_type":"stream","text":["+---------+\n","|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| :\u001b[43m \u001b[0m|B: |\n","+---------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1770XpkqLZ0e"},"source":["* **`env.reset`**: Resets the environment and returns a random initial state.\n","* **`env.step(action)`**: Step the environment by one timestep. Returns\n","    * **observation**: Observations of the environment\n","    * **reward**: If your action was beneficial or not\n","    * **done**: Indicates if we have successfully picked up and dropped off a passenger, also called one episode\n","    * **info**: Additional info such as performance and latency for debugging purposes\n","* **`env.render`**: Renders one frame of the environment (helpful in visualizing the environment)"],"id":"1770XpkqLZ0e"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCqIwcbdLZ0e","executionInfo":{"status":"ok","timestamp":1624353534723,"user_tz":-60,"elapsed":169,"user":{"displayName":"Ivana Dusparic","photoUrl":"","userId":"00662899544751628971"}},"outputId":"3e510f05-2cde-4257-e146-8913be8db01a"},"source":["env.reset()  # reset environment to a new, random state\n","env.render()\n","\n","print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))"],"id":"yCqIwcbdLZ0e","execution_count":4,"outputs":[{"output_type":"stream","text":["+---------+\n","|\u001b[35mR\u001b[0m: | : :G|\n","| : | : : |\n","| : : : :\u001b[43m \u001b[0m|\n","| | : | : |\n","|Y| : |\u001b[34;1mB\u001b[0m: |\n","+---------+\n","\n","Action Space Discrete(6)\n","State Space Discrete(500)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g3nPr0GeLZ0f"},"source":["* 0 = south\n","* 1 = north\n","* 2 = east\n","* 3 = west\n","* 4 = pickup\n","* 5 = dropoff"],"id":"g3nPr0GeLZ0f"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xmE1aVjdLZ0f","executionInfo":{"status":"ok","timestamp":1624353538309,"user_tz":-60,"elapsed":175,"user":{"displayName":"Ivana Dusparic","photoUrl":"","userId":"00662899544751628971"}},"outputId":"10536038-c036-4f1c-e346-b6e5c4a7d095"},"source":["state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n","print(\"State:\", state)\n","\n","env.s = state\n","env.render()"],"id":"xmE1aVjdLZ0f","execution_count":5,"outputs":[{"output_type":"stream","text":["State: 328\n","+---------+\n","|\u001b[35mR\u001b[0m: | : :G|\n","| : | : : |\n","| : : : : |\n","| |\u001b[43m \u001b[0m: | : |\n","|\u001b[34;1mY\u001b[0m| : |B: |\n","+---------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WmvkLhMyLZ0f","executionInfo":{"status":"ok","timestamp":1624354658442,"user_tz":-60,"elapsed":188,"user":{"displayName":"Ivana Dusparic","photoUrl":"","userId":"00662899544751628971"}},"outputId":"f3f2b18f-e6fe-4f88-c789-4067231ec1e1"},"source":["env.P[328]\n","\n","\n"],"id":"WmvkLhMyLZ0f","execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: [(1.0, 428, -1, False)],\n"," 1: [(1.0, 228, -1, False)],\n"," 2: [(1.0, 348, -1, False)],\n"," 3: [(1.0, 328, -1, False)],\n"," 4: [(1.0, 328, -10, False)],\n"," 5: [(1.0, 328, -10, False)]}"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"if6sJAIqLZ0g"},"source":["$$Q(state,action)\\leftarrow (1 - \\alpha)Q(state, action)+\\alpha(reward+\\gamma max_a Q(next state, all actions))$$\n","\n","Where:\n","\n","- $\\large\\alpha$ (alpha) is the learning rate ($0<\\alpha\\leq 1$) - Just like in supervised learning settings, Î± is the extent to which our Q-values are being updated in every iteration.\n","\n","- $\\large\\gamma$ (gamma) is the discount factor ($0\\leq\\gamma\\leq 1$) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy."],"id":"if6sJAIqLZ0g"},{"cell_type":"code","metadata":{"id":"C9Cvqsm8LZ0g","executionInfo":{"status":"ok","timestamp":1624353732462,"user_tz":-60,"elapsed":188,"user":{"displayName":"Ivana Dusparic","photoUrl":"","userId":"00662899544751628971"}}},"source":["q_table = np.zeros([env.observation_space.n, env.action_space.n])"],"id":"C9Cvqsm8LZ0g","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VV3RiJZPLZ0g","executionInfo":{"status":"ok","timestamp":1624353795713,"user_tz":-60,"elapsed":62153,"user":{"displayName":"Ivana Dusparic","photoUrl":"","userId":"00662899544751628971"}},"outputId":"c9523c08-b08e-4a4e-809b-b3b4493db419"},"source":["# Hyperparameters\n","alpha = 0.1\n","gamma = 0.99\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","seed= 42\n","rng =np.random.default_rng(seed)\n","\n","for i in range(1, 100001):\n","    state = env.reset()\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","    \n","    while not done:\n","        if rng.random() < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        next_state, reward, done, info = env.step(action) \n","        \n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","        \n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","        q_table[state, action] = new_value\n","\n","              \n","        if reward == -10:\n","            penalties += 1\n","\n","        state = next_state\n","        epochs += 1\n","        \n","    if i % 1000 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"],"id":"VV3RiJZPLZ0g","execution_count":8,"outputs":[{"output_type":"stream","text":["Episode: 100000\n","Training finished.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zn_WLkJXLZ0h","executionInfo":{"status":"ok","timestamp":1624353802292,"user_tz":-60,"elapsed":15,"user":{"displayName":"Ivana Dusparic","photoUrl":"","userId":"00662899544751628971"}},"outputId":"b4ff98e3-2d35-4747-8927-d10abe9bc062"},"source":["\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n","\n","total_epochs, total_penalties = 0, 0\n","total_reward = 0\n","episodes = 100\n","\n","for _ in range(episodes):\n","    state = env.reset()  # reset environment to a new, random state\n","    epochs, penalties, reward = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        action = np.argmax(q_table[state])\n","        state, reward, done, info = env.step(action)\n","        total_reward+=reward;\n","        if reward == -10:\n","            penalties += 1\n","\n","        epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")\n","print(f\"Average reward per episode: {total_reward / episodes}\")"],"id":"Zn_WLkJXLZ0h","execution_count":10,"outputs":[{"output_type":"stream","text":["Results after 100 episodes:\n","Average timesteps per episode: 13.13\n","Average penalties per episode: 0.0\n","Average reward per episode: 7.87\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wJguIfjzLZ0h"},"source":["Hyperparameters and optimizations\n","The values of ($\\large\\alpha$) `alpha`, ($\\large\\gamma$) `gamma`, and ($\\large\\epsilon$) `epsilon` were mostly based on intuition and some \"hit and trial\", but there are better ways to come up with good values.\n","\n","Ideally, all three should decrease over time because as the agent continues to learn, it actually builds up more resilient priors;\n","\n","$\\Large\\alpha$: (the learning rate) should decrease as you continue to gain a larger and larger knowledge base.\n","\n","$\\Large\\gamma$: as you get closer and closer to the deadline, your preference for near-term reward should increase, as you won't be around long enough to get the long-term reward, which means your gamma should decrease.\n","\n","$\\Large\\epsilon$: as we develop our strategy, we have less need of exploration and more exploitation to get more utility from our policy, so as trials increase, epsilon should decrease."],"id":"wJguIfjzLZ0h"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-p_Mfh7LZ0i","executionInfo":{"status":"ok","timestamp":1624354195659,"user_tz":-60,"elapsed":8088,"user":{"displayName":"Ivana Dusparic","photoUrl":"","userId":"00662899544751628971"}},"outputId":"a869461d-5f1b-4ab6-b440-c726e8a8152d"},"source":["# Play an episode\n","\n","state = env.reset()  # reset environment to a new, random state\n","env.render()\n","time.sleep(0.5)\n","done = False\n","\n","\n","while not done:\n","    action = np.argmax(q_table[state])\n","    state, reward, done, info = env.step(action)\n","    clear_output(wait=True)\n","    env.render()\n","    print(reward)\n","    time.sleep(0.5)\n","\n","  "],"id":"A-p_Mfh7LZ0i","execution_count":27,"outputs":[{"output_type":"stream","text":["+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n","+---------+\n","  (Dropoff)\n","20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XgAyx9ZaLZ0i"},"source":[""],"id":"XgAyx9ZaLZ0i","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hB0kSONuLZ0i"},"source":[""],"id":"hB0kSONuLZ0i","execution_count":null,"outputs":[]}]}