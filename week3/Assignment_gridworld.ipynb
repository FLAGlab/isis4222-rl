{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a0a6b89-5f5b-4706-8da8-c8d00ff69870",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gridworld y su solución como MDPs\n",
    "\n",
    "En este trabajo definiremos el ambiente de Gridworld y su solución como un MDP.\n",
    "Gridworld es un ambiente clásico de prueba dentro del aprendizaje por refuerzo. Durante este taller definiremos el modelo básico del ambiente, que extenderemos incrementalmente de acuerdo a las necesidades del algoritmo de solución.\n",
    "\n",
    "## Ambiente 🌎\n",
    "\n",
    "El ambiente de ridworld se define como una cuadricula de `nxm`. El ambiente tiene obstaculos, es decir casillas por las cuales no puede pasar el agente. Al chocar con un obstaculo, el agente se mantiene terminaría en el mismo estado inicial. Además, el ambiente tiene una casilla de inicio, y algunas casillas de salida. Un ejemplo del ambiente para el caso `3x4` se muestra a continuación.\n",
    "\n",
    "![gridworld.png](./img/gridworld.png)\n",
    "\n",
    "En este ejemplo del ambiente el agente comienza en la casilla inferior izquierda y tiene como objetivo llegar a la casilla de salida verde, con recompensa 1. La otra casilla de salida, tiene recompensa -1.\n",
    "\n",
    "\n",
    "### Task 1.\n",
    "#### ¿Cómo podemos codificar el ambiente?\n",
    "\n",
    "De una definición completa del ambiente, como una clase de python llamada `Environment`, estableciendo:\n",
    "1. Un atributo que define la cuadrícula (`board`). El ambiente recibirá una matriz como parámetro describiendo la cuadrícula en el momento de su creación. Definiremos las casillas por las que puede pasar el agente como casillas vacias, las casillas por las que no puede pasar el agente con un valor none `None` y las casillas de salida con el valor asociado a la recompensa definidas para cada una de ellas.\n",
    "2. Un atributo `nrows` para almacenar la cantidad de filas de la cuadrícula.\n",
    "3. Un atributo `ncols` para almacenar la cantidad de columnas de la cuadrícula.\n",
    "4. Un atributo `initial_state` para almacenar el estado inicial del agente dentro del ambiente.\n",
    "5. Un atributo con el estado actual (`current_state`) en el que se encuentra el agente. El valor de `current_state` se definirá como una tupla \n",
    "\n",
    "Un ejemplo de la definición del tablero para el caso de 5x5 de la figura anterior se da a continuación.\n",
    "```\n",
    "board = [['', ' ', ' ',  '+1'],\n",
    "         [' ', '#', ' ',  '-1'],\n",
    "         ['S', ' ', ' ', ' ']]\n",
    "```\n",
    "En el ejemplo `S` denota el estado inicial y `'#'` la casilla prohibida (manejaremos esta convención para todos los ambientes de gridworld).\n",
    "\n",
    "#### Comportamiento del ambeinte\n",
    "\n",
    "Una vez definido el ambiente definimos su comportamiento. Para ello requerimos los siguientes métodos:\n",
    "1. `get_current_state` que no recibe parámetros y retorna el estado actual (la casilla donde se encuentra el agente)\n",
    "2. `get_posible_actions` que recibe el estado actual del agente como parámetro y retorna las acciones disponibles para dicho estado. Las acciones estarán dadas por su nombre (`'up', 'down', 'left', 'right'`). Como convención definiremos que el agente siempre puede moverse en todas las direcciones, donde un movimiento en dirección de un obstáculo o los límites del ambiente no tienen ningún efecto visible en la posición del agente.\n",
    "3. `do_action` que recibe como parámetro la acción a ejecutar y retorna el valor de la recompensa y el nuevo estado del agente, como un pareja `reward, new_state`\n",
    "4. `reset` que no recibe parámetros y restablece el ambiente a su estado inicial.\n",
    "5. `is_terminal` que no recibe parámetros y determina si el agente está en el estado final o no. En nuestro caso, el estado final estará determinado por las casillas de salida (i.e., con un valor definido).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598cc01e-d7a7-4cb5-b72b-2d2adf013c9b",
   "metadata": {},
   "source": [
    "Teniendo en cuenta la definición del agente, genere un ambiente de `10x10` como se muestra a continuación.\n",
    "\n",
    "![evaluacion.png](./img/evaluacion.png)\n",
    "\n",
    "### Task 2.\n",
    "Plantee el problema de MDP para cada una de las casillas. Especifique el estado de inicio, las transiciones y su probabilidad (suponiendo que todas las acciones sucede con probabilidad de 0.25) y los estados de fin con su recompensa.\n",
    "¿Cómo serían las recompensas esperadas para cada estado?\n",
    "\n",
    "### Task 3.\n",
    "Bajo la definción del problema anterior, suponga que cada acción tiene una probabilidad de éxito de 60%, con probabilidad de 30% se ejecutará la sigiente acción (en dirección de las manesillas del reloj) y con probabilidad de 10% no pasará nada. Bajo estas condiciones, ¿Cómo serían las recompensas esperadas para cada estado? \n",
    "\n",
    "### Task 4. \n",
    "Defina una situación de la vide real, de su escogencia, como un MDP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
