{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a0a6b89-5f5b-4706-8da8-c8d00ff69870",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gridworld y su soluci칩n como MDPs\n",
    "\n",
    "En este trabajo definiremos el ambiente de Gridworld y su soluci칩n como un MDP.\n",
    "Gridworld es un ambiente cl치sico de prueba dentro del aprendizaje por refuerzo. Durante este taller definiremos el modelo b치sico del ambiente, que extenderemos incrementalmente de acuerdo a las necesidades del algoritmo de soluci칩n.\n",
    "\n",
    "## Ambiente 游깵\n",
    "\n",
    "El ambiente de ridworld se define como una cuadricula de `nxm`. El ambiente tiene obstaculos, es decir casillas por las cuales no puede pasar el agente. Al chocar con un obstaculo, el agente se mantiene terminar칤a en el mismo estado inicial. Adem치s, el ambiente tiene una casilla de inicio, y algunas casillas de salida. Un ejemplo del ambiente para el caso `3x4` se muestra a continuaci칩n.\n",
    "\n",
    "![gridworld.png](./img/gridworld.png)\n",
    "\n",
    "En este ejemplo del ambiente el agente comienza en la casilla inferior izquierda y tiene como objetivo llegar a la casilla de salida verde, con recompensa 1. La otra casilla de salida, tiene recompensa -1.\n",
    "\n",
    "\n",
    "### Task 1.\n",
    "#### 쮺칩mo podemos codificar el ambiente?\n",
    "\n",
    "De una definici칩n completa del ambiente, como una clase de python llamada `Environment`, estableciendo:\n",
    "1. Un atributo que define la cuadr칤cula (`board`). El ambiente recibir치 una matriz como par치metro describiendo la cuadr칤cula en el momento de su creaci칩n. Definiremos las casillas por las que puede pasar el agente como casillas vacias, las casillas por las que no puede pasar el agente con un valor none `None` y las casillas de salida con el valor asociado a la recompensa definidas para cada una de ellas.\n",
    "2. Un atributo `nrows` para almacenar la cantidad de filas de la cuadr칤cula.\n",
    "3. Un atributo `ncols` para almacenar la cantidad de columnas de la cuadr칤cula.\n",
    "4. Un atributo `initial_state` para almacenar el estado inicial del agente dentro del ambiente.\n",
    "5. Un atributo con el estado actual (`current_state`) en el que se encuentra el agente. El valor de `current_state` se definir치 como una tupla \n",
    "\n",
    "Un ejemplo de la definici칩n del tablero para el caso de 5x5 de la figura anterior se da a continuaci칩n.\n",
    "```\n",
    "board = [['', ' ', ' ',  '+1'],\n",
    "         [' ', '#', ' ',  '-1'],\n",
    "         ['S', ' ', ' ', ' ']]\n",
    "```\n",
    "En el ejemplo `S` denota el estado inicial y `'#'` la casilla prohibida (manejaremos esta convenci칩n para todos los ambientes de gridworld).\n",
    "\n",
    "#### Comportamiento del ambeinte\n",
    "\n",
    "Una vez definido el ambiente definimos su comportamiento. Para ello requerimos los siguientes m칠todos:\n",
    "1. `get_current_state` que no recibe par치metros y retorna el estado actual (la casilla donde se encuentra el agente)\n",
    "2. `get_posible_actions` que recibe el estado actual del agente como par치metro y retorna las acciones disponibles para dicho estado. Las acciones estar치n dadas por su nombre (`'up', 'down', 'left', 'right'`). Como convenci칩n definiremos que el agente siempre puede moverse en todas las direcciones, donde un movimiento en direcci칩n de un obst치culo o los l칤mites del ambiente no tienen ning칰n efecto visible en la posici칩n del agente.\n",
    "3. `do_action` que recibe como par치metro la acci칩n a ejecutar y retorna el valor de la recompensa y el nuevo estado del agente, como un pareja `reward, new_state`\n",
    "4. `reset` que no recibe par치metros y restablece el ambiente a su estado inicial.\n",
    "5. `is_terminal` que no recibe par치metros y determina si el agente est치 en el estado final o no. En nuestro caso, el estado final estar치 determinado por las casillas de salida (i.e., con un valor definido).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598cc01e-d7a7-4cb5-b72b-2d2adf013c9b",
   "metadata": {},
   "source": [
    "Teniendo en cuenta la definici칩n del agente, genere un ambiente de `10x10` como se muestra a continuaci칩n.\n",
    "\n",
    "![evaluacion.png](./img/evaluacion.png)\n",
    "\n",
    "### Task 2.\n",
    "Plantee el problema de MDP para cada una de las casillas. Especifique el estado de inicio, las transiciones y su probabilidad (suponiendo que todas las acciones sucede con probabilidad de 0.25) y los estados de fin con su recompensa.\n",
    "쮺칩mo ser칤an las recompensas esperadas para cada estado?\n",
    "\n",
    "### Task 3.\n",
    "Bajo la definci칩n del problema anterior, suponga que cada acci칩n tiene una probabilidad de 칠xito de 60%, con probabilidad de 30% se ejecutar치 la sigiente acci칩n (en direcci칩n de las manesillas del reloj) y con probabilidad de 10% no pasar치 nada. Bajo estas condiciones, 쮺칩mo ser칤an las recompensas esperadas para cada estado? \n",
    "\n",
    "### Task 4. \n",
    "Defina una situaci칩n de la vide real, de su escogencia, como un MDP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
