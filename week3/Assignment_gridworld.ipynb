{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a0a6b89-5f5b-4706-8da8-c8d00ff69870",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gridworld y su solución como MDPs\n",
    "\n",
    "## Ambiente 🌎\n",
    "\n",
    "Para la definición del agente manejaremos una cuadricula de `nxn` como nuestro ambiente. El ambiente tiene obstaculos, es decir casillas por las cuales no se puede pasar, una casilla de inicio, y algunas casillas de terminación. Este ambiente, es conocido en el aprendizaje por refuerzo como el ambiente de Gridworld. Un ejemplo del ambiente para el caso `5x5` se muestra a continuación.\n",
    "\n",
    "![gridworld.png]()\n",
    "\n",
    "Een este ejemplo del ambiente el agente comienza en la casilla inferior izquierda y tiene como objetivo llegar a la casilla de salida verde, con recompensa 1. La otra casilla de salida, tiene recompensa -1\n",
    "\n",
    "\n",
    "#### ¿Cómo podemos codificar el ambiente?\n",
    "\n",
    "**Task 1.** Complete la definición del ambiente estableciendo por:\n",
    "1. La cuadrícula (`board`), con las dimensiones dadas por parámetro. Definiremos las casillas por las que puede pasar el agente como espacios en blanco y las casillas por las que no puede pasar el agente con un `'*'`.\n",
    "2. Un atributo (`dimensions`) para almacenar las dimensiones del tablero\n",
    "3. Las recompensas de cada casilla de la cuadrícula estan definidas dentro de la definición de `board`.\n",
    "    - +1 para la casilla objetivo \n",
    "    - -1 para las casillas de trampa\n",
    "4. Un atributo con el estado actual (`current_state`) en el que se encuentra el agente. Por defecto este estado será la posición (0,0).\n",
    "\n",
    "#### Comportamiento del ambeinte\n",
    "\n",
    "Una vez definido el ambiente definimos su comportamiento. Para ello defina los métodos\n",
    "1. `get_current_state` que retorna el estado actual (la casilla donde se encuentra el agente)\n",
    "2. `get_posible_actions` que retorna las acciones disponibles para cada estado, dado por parámetro como una tupla `(i,j)`. Las acciones estarán dadas por su nombre (`'up', 'down', 'left', 'right'`)\n",
    "3. `do_action` que recibe como parámetro la acción a ejecutar y retorna el valor de la recompensa y el nuevo estado del agente, como un pareja `(reward, new_state)`\n",
    "4. `reset` que restablece el ambiente a su estado inicial (en nuestro caso moviendo al agente a la posición (0, 0))\n",
    "5. `is_terminal` que termina si está en el estado final o no. en nuestro caso el estado final estará determinado por la casilla con recompensa igual a 1.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598cc01e-d7a7-4cb5-b72b-2d2adf013c9b",
   "metadata": {},
   "source": [
    "Teniendo en cuenta la definición del agente, genere un ambiente de `10x10` como se muestra a continuación.\n",
    "\n",
    "![evaluacion.png](attachment:083ff53c-d05a-41bd-96fd-33dc7a61eff2.png)\n",
    "\n",
    "**Task 2.** \n",
    "Evalue cada una de las casillas para cada uno de los estados como un MDP. Especifique el estado de inicio, acción y estado de fin (suponiendo que todas las acciones sucede con probabilidad de 0.25).\n",
    "Como serían las recompensas esperadas para cada estado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
