{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA: State-Action-Reward-State-Action\n",
    "\n",
    "En este ejercicio vamos a implementar el algoritmo de SARSA como un ejemplo de los métodos on-policy para el aprendizaje por refuerzo. Esto es, crear agentes que aprenden a alcanzar un objetivo específico.\n",
    "\n",
    "El método de SARSA se basa en el cálculo de los q-valores utilizando los valores calculados para el estado de llegada siguiendo la fórmula de actualización de los q-valores:\n",
    "\n",
    "$Q(s,a) \\leftarrow (1-\\alpha)Q(s,a) + \\alpha[R(s) + \\gamma Q(s',a')] $\n",
    "\n",
    "Para implementar SARSA definiremos un agente, `sarsa_agent.py` el cual utilizaremos para interactuar con el ambiente de Gridworld.\n",
    "\n",
    "**Task 1**\n",
    "1.\tImplemente la classe `SARSA` con cinco atributos:\n",
    "    - `epsilon` que corresponde a la estrategia de aprendizaje $\\epsilon$-greedy, `0.9` por defecto.\n",
    "    - `gamma` que corresponde al factor de decuento a utilizar, `0.96` por defecto.\n",
    "    - `alpha` que corresponde a la taza de aprendizajem `0.81` por defecto.\n",
    "    - `Q` que almacena los q-valores del agente.\n",
    "    - `env` que es una referencia al ambiente.\n",
    "\n",
    "2. El comportamiento del agente (la interacción con el ambiente) esta dado por los métodos:\n",
    "    - `choose_action` que recibe un estado como parámetro y retorna la acción a ejecutar para dicho estado siguiendo una estrategia $\\epsilon$-greedy.\n",
    "    - `action_function` que recibe como parámetro los componentes de SARSA (estado1, acción1, recompensa, estado2, acción2) y calcula el q-valor `Q(estdo1, acción1)`.\n",
    "\n",
    "3. La interacción entre el agente y el ambiente inicia desde el ambiente, que ejecuta cada interacción de SARSA para cada episodio.\n",
    "(1) La interacción comienza decidiendo la acción a tomar para el estado actual (la cual esta dada por el agente), (2) luego debemos ejecutar la acción, obteniendo el estado de llegada y la recompensa de ejecutar dicha acción, (3) luego calculamos la acción a tomar para el estado de llegada, (4) por último calculamos el q-valor definido por la función de las acciones.\n",
    "\n",
    "**Task 2**\n",
    "Reproduzca el ambiente de frozen lake realizado en clase y responda las siguientes preguntas.\n",
    "1. ¿Cuál es el comportamiento del agente si utilizamos un factor de descuento de 1?\n",
    "2. ¿Cómo podemos minimizar la trayectoria del agente entre el estado inicial y el estado de llegada?\n",
    "\n",
    "Justifique sus respuestas con ejecuciones reales del agente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
