{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DFL TRPO: REINFORCE CartPole Exercise","version":"0.3.2","provenance":[{"file_id":"0Bz6XXZ1741KYazg5MVduT3diNVk","timestamp":1502574483697},{"file_id":"0B0YZgox9lf2HbFIySThTYkRPeW8","timestamp":1501867465076},{"file_id":"0Bz6XXZ1741KYUXRsNUI0c18wQVU","timestamp":1494350731563}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"iTueGwFWcDK2","colab_type":"text"},"source":["# REINFORCE CartPole Exercise\n","\n","*Contributors and Authors*: [**Ethan Holly**](https://www.linkedin.com/in/ethan-holly-3a181539/) wrote this fantastic guide to REINFORCE and the solution in Colab form, [**Charles Weill**](https://www.linkedin.com/in/charles-weill-9426323a/) implemented environment visualization for the Colab, and [Surya Bhupatiraju]() ported it for public use for Depth-First Learning (DFL). \n","\n","*NOTE: The solution to this exercise (along with very pretty visualizations) is [here](https://drive.google.com/file/d/1UFWxcu42B_x262NbrUwTH-r7A3BCrfm2/view?usp=sharing). Please give this exercise an earnest attempt before peeking at the solutions!*  \n","\n","---\n","\n","In this exercise, you will implement and experiment with a policy gradient RL algorithm and environment.\n","\n","The environment is CartPole: from OpenAI Gym https://gym.openai.com/envs/CartPole-v0. \n","\n","\n","## Exercise\n","\n","The following code blocks contain the parts of the REINFORCE algorithm. Fill in the parts marked TODO, and then run the training block below. Training should take up to 2 min, and your performance curve should approach 200 reward per episode. Then investigate the questions next to the training block.\n","\n","## Extra Challenges:\n","\n","* Once you finish this exercise, try to **swap out CartPole-v0 for Pendulum-v0**. CartPole is a simple discrete-action environment, while Pendulum is a simple continuous-action environment. You will need to make some modifications to the code to make it work. Try it out!\n","* Try to implement some more complex policy gradient features:\n","  * **Actor-Critic**: Uses a neural network (or other function approximation) instead of the empirical return. This has the effect of smoothing out the return estimates for a lower-variance policy gradient estimate. However, the estimate is now biased by the inaccuracy in the \"Critic\" network. It can also do some neat things like sharing hidden layers with the policy, which can speed up training. See: https://arxiv.org/abs/1602.01783\n","  * **Generalized Advantage Estimation (GAE)**: Uses a weighted sum of advantage estimates over future timesteps. It uses the lambda parameter for trading of between short-term and long-term dependencies, which trades off between bias and variance in policy gradient estimates. For an intuitive explanation, see: http://www.breloff.com/DeepRL-OnlineGAE/\n","  * **Proximal Policy Optimization (PPO)**: Policy gradients are an on-policy algorithm, which means that you can only update a policy with data sampled from that policy - otherwize the state distribution wouldn't match and you would have a poor estimate of the expected return! However, we can use importance sampling to correct for this drift in sampling, and use one batch of data to do multiple updates to the policy. PPO is an algorithm that does this effectively, while maintaining stable training. See: https://blog.openai.com/openai-baselines-ppo/#ppo"]},{"cell_type":"code","metadata":{"id":"1VacJd0VNcb3","colab_type":"code","cellView":"both","colab":{}},"source":["# @title (Run This) Imports\n","import os\n","import gym\n","import sys\n","from gym import spaces\n","import numpy as np\n","import pyglet\n","pyglet.options['search_local_libs']=False\n","pyglet.options['shadow_window']=False\n","from pyglet.window import xlib\n","xlib._have_utf8 = False\n","import pyglet.window\n","import tensorflow as tf\n","from google3.third_party.tensorflow.contrib import slim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2LwPk69SQOkn","colab_type":"code","cellView":"form","colab":{}},"source":["# @title (Run This) Visualization Source Code\n","# Source: https://github.com/jakevdp/JSAnimation\n","import os\n","import sys\n","import random\n","import string\n","import warnings\n","if sys.version_info < (3, 0):\n","    from cStringIO import StringIO as InMemory\n","else:\n","    from io import BytesIO as InMemory\n","from matplotlib.animation import writers, FileMovieWriter\n","from base64 import b64encode\n","\n","\n","JS_INCLUDE = \"\"\"\n","<script language=\"javascript\">\n","  /* Define the Animation class */\n","  function Animation(frames, img_id, slider_id, interval, loop_select_id){\n","    this.img_id = img_id;\n","    this.slider_id = slider_id;\n","    this.loop_select_id = loop_select_id;\n","    this.interval = interval;\n","    this.current_frame = 0;\n","    this.direction = 0;\n","    this.timer = null;\n","    this.frames = new Array(frames.length);\n","\n","    for (var i=0; i<frames.length; i++)\n","    {\n","     this.frames[i] = new Image();\n","     this.frames[i].src = frames[i];\n","    }\n","    document.getElementById(this.slider_id).max = this.frames.length - 1;\n","    this.set_frame(this.current_frame);\n","  }\n","\n","  Animation.prototype.get_loop_state = function(){\n","    var button_group = document[this.loop_select_id].state;\n","    for (var i = 0; i < button_group.length; i++) {\n","        var button = button_group[i];\n","        if (button.checked) {\n","            return button.value;\n","        }\n","    }\n","    return undefined;\n","  }\n","\n","  Animation.prototype.set_frame = function(frame){\n","    this.current_frame = frame;\n","    document.getElementById(this.img_id).src = this.frames[this.current_frame].src;\n","    document.getElementById(this.slider_id).value = this.current_frame;\n","  }\n","\n","  Animation.prototype.next_frame = function()\n","  {\n","    this.set_frame(Math.min(this.frames.length - 1, this.current_frame + 1));\n","  }\n","\n","  Animation.prototype.previous_frame = function()\n","  {\n","    this.set_frame(Math.max(0, this.current_frame - 1));\n","  }\n","\n","  Animation.prototype.first_frame = function()\n","  {\n","    this.set_frame(0);\n","  }\n","\n","  Animation.prototype.last_frame = function()\n","  {\n","    this.set_frame(this.frames.length - 1);\n","  }\n","\n","  Animation.prototype.slower = function()\n","  {\n","    this.interval /= 0.7;\n","    if(this.direction > 0){this.play_animation();}\n","    else if(this.direction < 0){this.reverse_animation();}\n","  }\n","\n","  Animation.prototype.faster = function()\n","  {\n","    this.interval *= 0.7;\n","    if(this.direction > 0){this.play_animation();}\n","    else if(this.direction < 0){this.reverse_animation();}\n","  }\n","\n","  Animation.prototype.anim_step_forward = function()\n","  {\n","    this.current_frame += 1;\n","    if(this.current_frame < this.frames.length){\n","      this.set_frame(this.current_frame);\n","    }else{\n","      var loop_state = this.get_loop_state();\n","      if(loop_state == \"loop\"){\n","        this.first_frame();\n","      }else if(loop_state == \"reflect\"){\n","        this.last_frame();\n","        this.reverse_animation();\n","      }else{\n","        this.pause_animation();\n","        this.last_frame();\n","      }\n","    }\n","  }\n","\n","  Animation.prototype.anim_step_reverse = function()\n","  {\n","    this.current_frame -= 1;\n","    if(this.current_frame >= 0){\n","      this.set_frame(this.current_frame);\n","    }else{\n","      var loop_state = this.get_loop_state();\n","      if(loop_state == \"loop\"){\n","        this.last_frame();\n","      }else if(loop_state == \"reflect\"){\n","        this.first_frame();\n","        this.play_animation();\n","      }else{\n","        this.pause_animation();\n","        this.first_frame();\n","      }\n","    }\n","  }\n","\n","  Animation.prototype.pause_animation = function()\n","  {\n","    this.direction = 0;\n","    if (this.timer){\n","      clearInterval(this.timer);\n","      this.timer = null;\n","    }\n","  }\n","\n","  Animation.prototype.play_animation = function()\n","  {\n","    this.pause_animation();\n","    this.direction = 1;\n","    var t = this;\n","    if (!this.timer) this.timer = setInterval(function(){t.anim_step_forward();}, this.interval);\n","  }\n","\n","  Animation.prototype.reverse_animation = function()\n","  {\n","    this.pause_animation();\n","    this.direction = -1;\n","    var t = this;\n","    if (!this.timer) this.timer = setInterval(function(){t.anim_step_reverse();}, this.interval);\n","  }\n","</script>\n","\"\"\"\n","\n","\n","DISPLAY_TEMPLATE = \"\"\"\n","<div class=\"animation\" align=\"center\">\n","    <img id=\"_anim_img{id}\">\n","    <br>\n","    <input id=\"_anim_slider{id}\" type=\"range\" style=\"width:350px\" name=\"points\" min=\"0\" max=\"1\" step=\"1\" value=\"0\" onchange=\"anim{id}.set_frame(parseInt(this.value));\"></input>\n","    <br>\n","    <button onclick=\"anim{id}.slower()\">&#8211;</button>\n","    <button onclick=\"anim{id}.first_frame()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/first.png?raw=true\"></button>\n","    <button onclick=\"anim{id}.previous_frame()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/prev.png?raw=true\"></button>\n","    <button onclick=\"anim{id}.reverse_animation()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/reverse.png?raw=true\"></button>\n","    <button onclick=\"anim{id}.pause_animation()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/pause.png?raw=true\"></button>\n","    <button onclick=\"anim{id}.play_animation()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/play.png?raw=true\"></button>\n","    <button onclick=\"anim{id}.next_frame()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/next.png?raw=true\"></button>\n","    <button onclick=\"anim{id}.last_frame()\"><img class=\"anim_icon\" src=\"https://github.com/jakevdp/JSAnimation/blob/master/JSAnimation/icons/last.png?raw=true\"></button>\n","    <button onclick=\"anim{id}.faster()\">+</button>\n","  <form action=\"#n\" name=\"_anim_loop_select{id}\" class=\"anim_control\">\n","    <input type=\"radio\" name=\"state\" value=\"once\" {once_checked}> Once </input>\n","    <input type=\"radio\" name=\"state\" value=\"loop\" {loop_checked}> Loop </input>\n","    <input type=\"radio\" name=\"state\" value=\"reflect\" {reflect_checked}> Reflect </input>\n","  </form>\n","</div>\n","\n","\n","<script language=\"javascript\">\n","  /* Instantiate the Animation class. */\n","  /* The IDs given should match those used in the template above. */\n","  (function() {{\n","    var img_id = \"_anim_img{id}\";\n","    var slider_id = \"_anim_slider{id}\";\n","    var loop_select_id = \"_anim_loop_select{id}\";\n","    var frames = new Array({Nframes});\n","    {fill_frames}\n","\n","    /* set a timeout to make sure all the above elements are created before\n","       the object is initialized. */\n","    setTimeout(function() {{\n","        anim{id} = new Animation(frames, img_id, slider_id, {interval}, loop_select_id);\n","    }}, 0);\n","  }})()\n","</script>\n","\"\"\"\n","\n","INCLUDED_FRAMES = \"\"\"\n","  for (var i=0; i<{Nframes}; i++){{\n","    frames[i] = \"{frame_dir}/frame\" + (\"0000000\" + i).slice(-7) + \".{frame_format}\";\n","  }}\n","\"\"\"\n","\n","\n","def _included_frames(frame_list, frame_format):\n","    \"\"\"frame_list should be a list of filenames\"\"\"\n","    return INCLUDED_FRAMES.format(Nframes=len(frame_list),\n","                                  frame_dir=os.path.dirname(frame_list[0]),\n","                                  frame_format=frame_format)\n","\n","\n","def _embedded_frames(frame_list, frame_format):\n","    \"\"\"frame_list should be a list of base64-encoded png files\"\"\"\n","    template = '  frames[{0}] = \"data:image/{1};base64,{2}\"\\n'\n","    embedded = \"\\n\"\n","    for i, frame_data in enumerate(frame_list):\n","        embedded += template.format(i, frame_format,\n","                                    frame_data.replace('\\n', '\\\\\\n'))\n","    return embedded\n","\n","\n","@writers.register('html')\n","class HTMLWriter(FileMovieWriter):\n","    # we start the animation id count at a random number: this way, if two\n","    # animations are meant to be included on one HTML page, there is a\n","    # very small chance of conflict.\n","    rng = random.Random()\n","    exec_key = 'animation.ffmpeg_path'\n","    args_key = 'animation.ffmpeg_args'\n","    supported_formats = ['png', 'jpeg', 'tiff', 'svg']\n","\n","    @classmethod\n","    def new_id(cls):\n","        #return '%16x' % cls.rng.getrandbits(64)\n","        return ''.join(cls.rng.choice(string.ascii_uppercase)\n","                       for x in range(16))\n","\n","    def __init__(self, fps=30, codec=None, bitrate=None, extra_args=None,\n","                 metadata=None, embed_frames=False, default_mode='loop'):\n","        self.embed_frames = embed_frames\n","        self.default_mode = default_mode.lower()\n","\n","        if self.default_mode not in ['loop', 'once', 'reflect']:\n","            self.default_mode = 'loop'\n","            warnings.warn(\"unrecognized default_mode: using 'loop'\")\n","\n","        self._saved_frames = list()\n","        super(HTMLWriter, self).__init__(fps, codec, bitrate,\n","                                         extra_args, metadata)\n","\n","    def setup(self, fig, outfile, dpi, frame_dir=None):\n","        if os.path.splitext(outfile)[-1] not in ['.html', '.htm']:\n","            raise ValueError(\"outfile must be *.htm or *.html\")\n","\n","        if not self.embed_frames:\n","            if frame_dir is None:\n","                frame_dir = outfile.rstrip('.html') + '_frames'\n","            if not os.path.exists(frame_dir):\n","                os.makedirs(frame_dir)\n","            frame_prefix = os.path.join(frame_dir, 'frame')\n","        else:\n","            frame_prefix = None\n","\n","        super(HTMLWriter, self).setup(fig, outfile, dpi,\n","                                      frame_prefix, clear_temp=False)\n","\n","    def grab_frame(self, **savefig_kwargs):\n","        if self.embed_frames:\n","            suffix = '.' + self.frame_format\n","            f = InMemory()\n","            self.fig.savefig(f, format=self.frame_format,\n","                             dpi=self.dpi, **savefig_kwargs)\n","            f.seek(0)\n","            self._saved_frames.append(b64encode(f.read()).decode('ascii'))\n","        else:\n","            return super(HTMLWriter, self).grab_frame(**savefig_kwargs)\n","\n","    def _run(self):\n","        # make a ducktyped subprocess standin\n","        # this is called by the MovieWriter base class, but not used here.\n","        class ProcessStandin(object):\n","            returncode = 0\n","            def communicate(self):\n","                return ('', '')\n","        self._proc = ProcessStandin()\n","\n","        # save the frames to an html file\n","        if self.embed_frames:\n","            fill_frames = _embedded_frames(self._saved_frames,\n","                                           self.frame_format)\n","        else:\n","            # temp names is filled by FileMovieWriter\n","            fill_frames = _included_frames(self._temp_names,\n","                                           self.frame_format)\n","\n","        mode_dict = dict(once_checked='',\n","                         loop_checked='',\n","                         reflect_checked='')\n","        mode_dict[self.default_mode + '_checked'] = 'checked'\n","\n","        interval = int(1000. / self.fps)\n","\n","        with open(self.outfile, 'w') as of:\n","            of.write(JS_INCLUDE)\n","            of.write(DISPLAY_TEMPLATE.format(id=self.new_id(),\n","                                             Nframes=len(self._temp_names),\n","                                             fill_frames=fill_frames,\n","                                             interval=interval,\n","                                             **mode_dict))\n","            \n","  # from .html_writer import HTMLWriter\n","from matplotlib.animation import Animation\n","import matplotlib.pyplot as plt\n","import tempfile\n","import random\n","import os\n","\n","\n","__all__ = ['anim_to_html', 'display_animation']\n","\n","\n","class _NameOnlyTemporaryFile(object):\n","    \"\"\"A context-managed temporary file which is not opened.\n","\n","    The file should be accessible by name on any system.\n","\n","    Parameters\n","    ----------\n","    suffix : string\n","        The suffix of the temporary file (default = '')\n","    prefix : string\n","        The prefix of the temporary file (default = '_tmp_')\n","    hash_length : string\n","        The length of the random hash.  The size of the hash space will\n","        be 16 ** hash_length (default=8)\n","    seed : integer\n","        the seed for the random number generator.  If not specified, the\n","        system time will be used as a seed.\n","    absolute : boolean\n","        If true, return an absolute path to a temporary file in the current\n","        working directory.\n","\n","    Example\n","    -------\n","\n","    >>> with _NameOnlyTemporaryFile(seed=0, absolute=False) as f:\n","    ...     print(f)\n","    ...\n","    _tmp_d82c07cd\n","    >>> os.path.exists('_tmp_d82c07cd')  # file removed after context\n","    False\n","\n","    \"\"\"\n","    def __init__(self, prefix='_tmp_', suffix='', hash_length=8,\n","                 seed=None, absolute=True):\n","        rng = random.Random(seed)\n","        self.name = '%s%0*x%s' % (prefix, hash_length,\n","                                  rng.getrandbits(4 * hash_length), suffix)\n","        if absolute:\n","            self.name = os.path.abspath(self.name)\n","\n","    def __enter__(self):\n","        return self\n","\n","    def __exit__(self, *exc_info):\n","        if os.path.exists(self.name):\n","            os.remove(self.name)\n","\n","\n","def anim_to_html(anim, fps=None, embed_frames=True, default_mode='loop'):\n","    \"\"\"Generate HTML representation of the animation\"\"\"\n","    if fps is None and hasattr(anim, '_interval'):\n","        # Convert interval in ms to frames per second\n","        fps = 1000. / anim._interval\n","\n","    plt.close(anim._fig)\n","    if hasattr(anim, \"_html_representation\"):\n","        return anim._html_representation\n","    else:\n","        # tempfile can't be used here: we need a filename, and this\n","        # fails on windows.  Instead, we use a custom filename generator\n","        #with tempfile.NamedTemporaryFile(suffix='.html') as f:\n","        with _NameOnlyTemporaryFile(suffix='.html') as f:\n","            anim.save(f.name,  writer=HTMLWriter(fps=fps,\n","                                                 embed_frames=embed_frames,\n","                                                 default_mode=default_mode))\n","            html = open(f.name).read()\n","\n","        anim._html_representation = html\n","        return html\n","\n","\n","def display_animation(anim, **kwargs):\n","    \"\"\"Display the animation with an IPython HTML object\"\"\"\n","    from IPython.display import HTML\n","    return HTML(anim_to_html(anim, **kwargs))\n","\n","\n","# This is the magic that makes animations display automatically in the\n","# IPython notebook.  The _repr_html_ method is a special method recognized\n","# by IPython.\n","Animation._repr_html_ = anim_to_html\n","\n","%matplotlib inline\n","# from JSAnimation.IPython_display import display_animation\n","import matplotlib.pyplot as plt\n","from IPython.display import display\n","from matplotlib import animation\n","\n","def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n","    display(display_animation(anim, default_mode='loop'))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sy4pCYqztJV4","colab_type":"text"},"source":["## DiscretePolicy\n","\n","This class implements a discrete policy that uses a neural network to parameterize a categorical distribution on a discrete set of actions.\n","\n","The neural net takes the observation vector as input, then should produce probabilities that can be fed to a categorical distribution.\n","\n","The parameters of this neural net will be trained below using the REINFORCE policy gradients algorithm."]},{"cell_type":"code","metadata":{"id":"5M5dlNDotHcA","colab_type":"code","colab":{}},"source":["class DiscretePolicy:\n","  \"\"\"Simple discrete-action policy with feed-forward net.\"\"\"\n","  \n","  def __init__(self,\n","               obs_dim,\n","               action_dim):\n","    \"\"\"Creates a discrete policy paramaterized by a neural network.\n","    \n","    Args:\n","      obs_dim: The dimension of observation vectors. For cartpole, this will\n","        be 4.\n","      action_dim: The number of discrete actions the policy has to choose from.\n","        for cartpole, this will be 2.\n","    \"\"\"\n","    self._obs_dim = obs_dim\n","    self._action_dim = action_dim\n","    self._make_net()\n","  \n","  def _make_net(self):\n","    # Make observation placeholder with batch dimension.\n","    with tf.variable_scope('discrete_policy'):\n","      self._obs_placeholder = tf.placeholder(dtype=tf.float32, \n","                                             shape=[None, self._obs_dim])\n","      \n","      # TODO: Create neural network layers from obs_placeholder that produce\n","      #         values for action_probabilities.\n","      # Hint: Use the tf.slim library to create basic neural network layers.\n","      # Hint: You can use tf.nn.softmax to turn arbitrary weights into probabilities that sum to 1.\n","      self._action_probabilities = ???\n","      \n","      # TODO: Use the action_probabilities to parameterize a TF distribution,\n","      #         which can be used to sample actions.\n","      # https://www.tensorflow.org/api_guides/python/contrib.distributions\n","      # Hint: The distribution to use should among discrete choices.\n","      self._action_distribution = ???\n","      \n","      self._action_sample = self._action_distribution.sample()\n","    \n","    # Set up references to variables for getting & setting params.\n","    self._variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discrete_policy')\n","    self._var_placeholders = []\n","    self._var_assigns = []\n","    for var in self._variables:\n","      var_ph = tf.placeholder(dtype=tf.float32, shape=var.shape)\n","      self._var_placeholders.append(var_ph)\n","      self._var_assigns.append(var.assign(var_ph))\n","    \n","  @property\n","  def action_dim(self):\n","    return self._action_dim\n","  \n","  @property\n","  def obs_placeholder(self):\n","    \"\"\"Property returns observation placeholder. shape=[batch_dim, obs_dim]\"\"\"\n","    return self._obs_placeholder\n","  \n","  @property\n","  def action_probabilities(self):\n","    \"\"\"Returns tensor of action probabilities. shape=[batch_dim, action_dim]\"\"\"\n","    return self._action_probabilities\n","  \n","  def get_params(self, sess=None):\n","    \"\"\"Return the parameter vals as an array of tensors, one for each param.\"\"\"\n","    sess = sess or tf.get_default_session()\n","    return sess.run(self._variables)\n","  \n","  def set_params(self, param_vals, sess=None):\n","    \"\"\"Sets policy variables equal to param_vals\"\"\"\n","    sess = sess or tf.get_default_session()\n","    return sess.run(self._var_assigns,\n","                    feed_dict={var_ph: param_val\n","                               for (var_ph, param_val)\n","                               in zip(self._var_placeholders, param_vals)})\n","  \n","  def sample_action(self, obs, sess=None):\n","    \"\"\"Samples an action from the policy distribution, given observation.\n","    \n","    Args:\n","      obs: Vector of observation values.\n","      sess: The session to use for execution. If not specified, uses\n","        tf.get_default_session().\n","    \"\"\"\n","    sess = sess or tf.get_default_session()\n","    sample = sess.run(self._action_sample,\n","                      feed_dict={self._obs_placeholder: [obs]})\n","    return sample[0] # remove batch index"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMrD3KrQv2fq","colab_type":"text"},"source":["## REINFORCE\n","\n","This class implements the REINFORCE training algorithm for maximizing a policy's expected return.\n","\n","It is constructed with the policy to train as well as hyperparameters for the learning algorithm.\n","\n","The train function takes a list of episodes that are used to compute the expected return of the current policy, which is used for the policy gradient update."]},{"cell_type":"code","metadata":{"id":"0D7eKG-vxD65","colab_type":"code","colab":{}},"source":["class REINFORCE:\n","  \"\"\"Implements REINFORCE training of a policy.\"\"\"\n","  \n","  def __init__(self,\n","               policy,\n","               value_function=None,\n","               learning_rate=1e-3,\n","               discount_rate=0.99,\n","               baseline_update_rate=0.01,\n","               estimation_algorithm='constant_baseline'):\n","    \"\"\"Constructs training ops for REINFORCE training of a policy.\n","    \n","    Args:\n","      policy: The discrete policy to train.\n","      value_function: The value function approximator to use for critic.\n","      learning_rate: The learning rate to use for optimizer.\n","      sess: The session to use for execution. If not specified, uses\n","        tf.get_default_session().\n","    \"\"\"\n","    self._policy = policy\n","    self._value_function = value_function\n","    self._learning_rate = learning_rate\n","    self._discount_rate = discount_rate\n","    self._baseline_update_rate = baseline_update_rate\n","    self._estimation_algorithm = estimation_algorithm\n","    \n","    with tf.variable_scope('REINFORCE'):\n","      # A one-hot mask of actions taken.\n","      self._action_placeholder = tf.placeholder(dtype=tf.float32,\n","                                                shape=[None, self._policy.action_dim],\n","                                                name='action_placeholder')\n","      # The per-timestep discounted return.\n","      self._return_placeholder = tf.placeholder(dtype=tf.float32,\n","                                                shape=[None, 1],\n","                                                name='return_placeholder')\n","\n","      # Use the action_placeholder mask, to select which actions were actually\n","      #   taken, then reduce along the action index, preserving the batch index.\n","      self._action_prob = tf.reduce_sum(self._action_placeholder * \n","                                        self._policy.action_probabilities,\n","                                        axis=1, keep_dims=True)\n","      \n","      # TODO: Define the policy gradient loss.\n","      # Hint: See slide 15 of ICML 2017 Deep RL lecture slides.\n","      #   https://sites.google.com/corp/view/icml17deeprl\n","      #\n","      # The policy gradient is shown in these slides to maximize the\n","      #   quantity log(pi(a_t|s_t)) * R_t.\n","      self._loss = ???\n","      \n","      self._optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n","\n","      # TODO: Create an op that uses the optimizer to minimize self._loss.\n","      self._train_op = ???\n","  \n","  def _estimate_returns(self,\n","                        episode,\n","                        algorithm,):\n","    \"\"\"Estimates the return to use for each timestep for policy gradient.\n","    \n","    Changes behavior based on algorithm specified.\n","      - no_baseline: Does not subtract a baseline from empirical return.\n","      - constant_baseline: Subtracts a constant baseline from empirical return.\n","          Updates the baseline used online to be a moving average of the\n","          observed return.\n","    \n","    Args:\n","      episode: Tuple of (observations, actions, rewards).\n","      algorithm: A string value indicating which algorithm to use for computing\n","        return / critic values.\n","    \"\"\"\n","    observations, actions, rewards = episode\n","    \n","    # TODO: Compute the return at each point in the episode, indeces aligned on\n","    #   timestep with the other arrays. The return for timestep t is r_t plus\n","    #   the discounted future return.\n","    returns = ???\n","    \n","    if algorithm == 'no_baseline':\n","      returns = [[R] for R in returns]\n","    elif algorithm == 'constant_baseline':\n","      # Subtract the baseline from the empirical returns.\n","      r_mean = np.mean(returns)\n","      r_std = np.std(returns)\n","      returns = [[(R - r_mean) / (r_std + 1e-6)] for R in returns]\n","      \n","    return returns\n","  \n","  def train(self, episodes, sess=None):\n","    \"\"\"Trains the DiscretePolicy with episodes of experience.\n","    \n","    Prepares observed actions as one-hot vectors and computes return at each\n","    timestep, used by the policy gradient update rule.\n","    \n","    Args:\n","      episodes: A list of tuples, each tuple of form\n","        (observations, actions, rewards).\n","    \"\"\"\n","    sess = sess or tf.get_default_session()\n","    \n","    # Observations, actions, returns. Timesteps aligned on indexes.\n","    observations_batch = []\n","    actions_batch = []\n","    returns_batch = []\n","    \n","    # For each episode, prepare the data. Computes returns, one-hot actions,\n","    #   and aggregates all episodes into a batch for tf graph execution.\n","    for episode in episodes:\n","      observations, actions, rewards = episode\n","      \n","      # Back up timesteps from end of episode to compute return from each t.\n","      returns = self._estimate_returns(episode,\n","                                       algorithm=self._estimation_algorithm,\n","                                       sess=sess)\n","      \n","      # TODO: Create one-hot representation of actions taken. Use the array\n","      #   'actions', and self._policy._action_dim.\n","      # Hint: Use numpy for simpler quick-and-dirty array manipulation. No need\n","      #   to use TensorFlow for this.\n","      one_hot_actions = ???\n","      \n","      # Add this episode of data to the minibatch.\n","      observations_batch.extend(observations)\n","      actions_batch.extend(one_hot_actions)\n","      returns_batch.extend(returns)\n","    \n","    sess.run(self._train_op,\n","             feed_dict={\n","                 self._policy.obs_placeholder: observations_batch,\n","                 self._action_placeholder: actions_batch,\n","                 self._return_placeholder: returns_batch, # foo\n","             })\n","    \n","    # If value function is not None, train it, too.\n","    if self._value_function is not None:\n","      returns_batch_no_baseline = []\n","      for episode in episodes:\n","        returns_batch_no_baseline.extend(self._estimate_returns(episode,\n","                                                                algorithm='no_baseline',\n","                                                                sess=sess))\n","      self._value_function.train(observations_batch, returns_batch, sess=sess)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OWYC5tBJyD-D","colab_type":"text"},"source":["## Collect Episode\n","\n","This function collects a single episode of data. It alternates between querying environment for observations and rewards, then policy for actions given observations."]},{"cell_type":"code","metadata":{"id":"qngwK9MkyB75","colab_type":"code","colab":{}},"source":["def collect_episode(env, policy, frames=None):\n","  \"\"\"Collect an episode of environment interaction.\n","  \n","  Args:\n","    env: The environment to interact with.\n","    policy: The policy to use for action selection.\n","    frames: Optional array to store the frames. If not provided, env does not\n","      bother rendering.\n","  Returns:\n","    Tuple of arrays of observations, actions, and rewards.\n","  \"\"\"\n","  obs = env.reset()\n","  if frames is not None:\n","    frames.append(env.render(mode='rgb_array'))\n","  done = False\n","  observations, actions, rewards = ([], [], [])\n","  while not done:\n","    observations.append(obs)\n","    action = policy.sample_action(obs)\n","    actions.append(action)\n","    obs, reward, done, _ = env.step(action)\n","    rewards.append(reward)\n","    if frames is not None:\n","      frames.append(env.render(mode='rgb_array'))\n","  return observations, actions, rewards"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FTVsIx_3xRNH","colab_type":"text"},"source":["## Running Training\n","\n","Before you can run training, you will need to fill in the TODO's in the above code blocks. Once you have filled them in correctly, the following training loop should successfully train your policy, and the output curve should approach an episode performance of 200 reward.\n","\n","### TODO: Questions\n","* What happens when you vary the learning rate parameter? Does the policy learn faster? Does it change stability?\n","* If you train for longer, does the policy get more stable? Does it reach a point where it never falls over? (score=200 is a perfect episode)\n","* What happens when you restart training and run again? How much variance is there across runs? (may need to reduce SLIDING_WINDOW_SIZE to see a difference)\n","* What happens when you change the estimation_algorithm parameter from 'no_baseline' to 'constant_baseline'? Compare the training curves qualitatively. Take a look at the REINFORCE implementation to see what's going on. What is happening?"]},{"cell_type":"code","metadata":{"id":"sF-ifxDyVPBG","colab_type":"code","cellView":"both","colab":{}},"source":["EXPERIMENT_NAME = 'reinforce' # @param\n","\n","tf.reset_default_graph()\n","sess = tf.Session()\n","\n","env = gym.make('CartPole-v0')\n","policy = DiscretePolicy(obs_dim=4, action_dim=2)\n","\n","LEARNING_RATE = 1e-3 # @param\n","reinforce = REINFORCE(policy,\n","                    learning_rate=LEARNING_RATE,\n","                    estimation_algorithm='no_baseline')\n","\n","sess.run(tf.initialize_all_variables())\n","\n","NUM_ITERATIONS = 1000 # @param\n","EPISODES_PER_ITERATION = 1 # @param\n","all_actions = []\n","\n","POLICY_SAVE_FREQUENCY = 20\n","\n","# Make dictionary for saving policy parameters.\n","try:\n","  policy_params[EXPERIMENT_NAME] = {}\n","except NameError:\n","  policy_params = {EXPERIMENT_NAME: {}}\n","\n","training_scores = []\n","with sess.as_default():\n","  for i in xrange(NUM_ITERATIONS):\n","    # Collect episodes.\n","    episodes = []\n","    for _ in xrange(EPISODES_PER_ITERATION):\n","      episodes.append(collect_episode(env,policy))\n","      \n","    # Train\n","    reinforce.train(episodes)\n","    \n","    # Sum rewards across episodes.\n","    avg_reward = sum(map(lambda episode: sum(episode[2]),\n","                           episodes)) / EPISODES_PER_ITERATION\n","    training_scores.append(avg_reward)\n","    sys.stdout.write('\\r{}/{} Iterations, {} Total reward this iteration.'.format(i+1, NUM_ITERATIONS, avg_reward))\n","    \n","    # Save intermittent policy parameters for running episodes later.\n","    if i % POLICY_SAVE_FREQUENCY == 0:\n","      policy_params[EXPERIMENT_NAME][i] = policy.get_params()        \n","  print('')\n","  policy_params[EXPERIMENT_NAME][NUM_ITERATIONS] = policy.get_params()\n","  \n","# Avg training_scores by sliding window.\n","SLIDING_WINDOW_SIZE = 30 # @param\n","training_scores = [np.mean(training_scores[max(i-SLIDING_WINDOW_SIZE, 0):i+1])\n","                   for i in xrange(len(training_scores))]\n","\n","# Save training_scores list by EXPERIMENT_NAME.\n","try:\n","  all_scores[EXPERIMENT_NAME] = training_scores\n","except NameError:\n","  all_scores = {EXPERIMENT_NAME: training_scores}\n","\n","# Plot all experiments so far.\n","experiment_names = all_scores.keys()\n","for name in experiment_names:\n","  plt.plot(all_scores[name])\n","plt.legend(experiment_names, loc='upper left')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSfei5kXeWmN","colab_type":"code","colab":{}},"source":["# @title Run this to clear scores.\n","try:\n","  del all_scores\n","  del policy_params\n","except NameError:\n","  pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6I_Mmp53scJK","colab_type":"text"},"source":["# Observing Episodes\n","\n","Load the policy parameters from different points in training, and rerun this code block to generate & visualize a new episode.\n","\n","NOTE: This will only work if you have started the colab kernel manually on your desktop machine. It will not work if you started it through an ssh connection."]},{"cell_type":"code","metadata":{"id":"8-9F1LbcwUq9","colab_type":"code","cellView":"code","colab":{}},"source":["experiment_name = 'reinforce' # @param\n","policy_version = 0 # @param\n","\n","# Round down to the last saved policy version.\n","policy_version = int(policy_version/POLICY_SAVE_FREQUENCY) * POLICY_SAVE_FREQUENCY\n","\n","render_env = gym.make('CartPole-v0')\n","\n","try:\n","  with sess.as_default():\n","    policy.set_params(policy_params[experiment_name][policy_version])\n","    frames = []\n","    collect_episode(render_env, policy, frames=frames)\n","    display_frames_as_gif(frames)\n","except Exception e:\n","  print ('DISPLAY ERROR: Cannot render episodes unless colab kernel was '\n","         'manually run from corp machine.')\n","  raise e\n","  \n","del render_env"],"execution_count":0,"outputs":[]}]}