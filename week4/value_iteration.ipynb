{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteración de Valores\n",
    "\n",
    "En este ejercicio vamos a implementar el primer método para solucionar Procesos de Decisión de Markov (MDPs). El método a implementar es la iteración de valores.\n",
    "\n",
    "La iteración de valores esta basada en la fórmula:\n",
    "\n",
    "![value_iteration](./img/value_iteration.png)\n",
    "\n",
    "Para resolver los MDPs crearemos un archivo `value_iteration.py` el cual utilizaremos para solucionar el ambiente de Gridworld.\n",
    "\n",
    "**Task 1**\n",
    "1.\tImplemente la classe `ValueIteration` con cuatro atributos:\n",
    "    - `mdp` que corresponde al MDP a resolver (e.g., Gridworld) \n",
    "    - `discount` que corresponde al factor de decuento a utilizar, `0.9` por defecto.\n",
    "    - `iterations` que corresponde a el número de iteraciones a realizar.\n",
    "    - `values` que corresponde a los valores calculados para los estados del MDP.\n",
    "\n",
    "2. El comportamiento del agente (de iteración de valores) esta dado por los métodos:\n",
    "    - `run_value_iteration` que no recibe ningún parámatetro y ejecuta el algoritmo de iteración de valores para la solución del MDP.\n",
    "    - `get_value` recibe un estado y retorna el valor correspondiende para dicho estado.\n",
    "    - `compute_qvalue_from_values` recibe un estado y una acción y calcula el q valor correspondiente.\n",
    "    - `compute_action_from_values` que calcula la acción a tomar (como la acción con el mejor valor en `values`) para un estado dado\n",
    "    - `get_action` retorna la acción a tomar dado un estado (directamente como la acción de la política, sin exploración)\n",
    "    - `get_qvalue` retorna el q valor dado un estado y una acción\n",
    "    - `get_policy` que retorna la acción a tomar para un estado (como `get_action`). Si el estado no tiene una acción asociada a él, retorne `None`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega\n",
    "\n",
    "Para esta tarea debe entregar: \n",
    "- La implementación de la iteración de valores para solucionar MDPs (`value_iteration.py`).\n",
    "- Un documento de análisis respondiendo a las siguientes preguntas (con screenshots de la solución y las explicaciones correspondientes del comportamiento observado).\n",
    "  -\tEjecute su implementación de iteración de valores para 5, 10, 15, 20, 30, 50 iteraciones, sobre el ambiente de gridworld. ¿Cuando convergen los valores o las acciones?\n",
    "  -\tEjecute su implementación sobre el ambiente del puente (i.e., Bridge) durante 10 iteraciones. Qué resultado observa si kodifica el valor de descuento de 0.9 a 0.1. Explique si los resultados cambian y porqué. \n",
    "  \n",
    "  Recuerde que el ambiente del puente se define con la matriz de `3x7` donde las filas 1 y 3 tienen recompensa -100 entre las columnas 2 y 6. La fila 2 corresponde a el puente, con entrada en la casilla `(2,1)` y salida en la casilla `(2,7)` con recompensa 100, como se muestra en la figura\n",
    "\n",
    "  ![bridge](./img/bridge.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
